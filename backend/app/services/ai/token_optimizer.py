"""
TokenèŠ‚çœä¼˜åŒ–æœåŠ¡ - é«˜æ æ†ç­–ç•¥å®ç°

åŠŸèƒ½ï¼š
1. å†…å®¹é¢„å¤„ç†å’Œæˆªæ–­
2. æ‰¹é‡å¤„ç†å‡å°‘APIè°ƒç”¨
3. æ™ºèƒ½é‡‡æ ·ç­–ç•¥
4. è¾“å‡ºæ ¼å¼ä¼˜åŒ–
5. é¢„ç®—æ§åˆ¶å’Œç†”æ–­

é’ˆå¯¹ç¡¬ä»¶é…ç½®ï¼ši5-11260H/16GB/RTX 3050 4GB
"""

import asyncio
import logging
import hashlib
import json
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass

from app.core.config import settings
from app.services.cache.redis_service import redis_service

logger = logging.getLogger(__name__)


@dataclass
class TokenBudget:
    """Tokené¢„ç®—ç®¡ç†"""
    daily_limit: int
    hourly_limit: int
    current_daily: int = 0
    current_hourly: int = 0
    last_reset_hour: int = 0
    last_reset_day: int = 0


class TokenOptimizer:
    """TokenèŠ‚çœä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.enabled = getattr(settings, 'TOKEN_SAVE_ENABLED', True)
        self.content_max_length = getattr(settings, 'TOKEN_SAVE_CONTENT_MAX_LENGTH', 500)
        self.reason_max_length = getattr(settings, 'TOKEN_SAVE_REASON_MAX_LENGTH', 50)
        self.batch_size = getattr(settings, 'TOKEN_SAVE_BATCH_SIZE', 10)
        self.sampling_rate = getattr(settings, 'TOKEN_SAVE_SAMPLING_RATE', 0.3)
        
        # é¢„ç®—æ§åˆ¶
        self.budget = TokenBudget(
            daily_limit=getattr(settings, 'CLOUD_DAILY_BUDGET_CALLS', 1000),
            hourly_limit=getattr(settings, 'CLOUD_HOURLY_BUDGET_CALLS', 100)
        )
        
        # æ‰¹å¤„ç†é˜Ÿåˆ—
        self._batch_queue: List[Dict[str, Any]] = []
        self._batch_lock = asyncio.Lock()
        
        logger.info(f"Tokenä¼˜åŒ–å™¨åˆå§‹åŒ–: enabled={self.enabled}, sampling_rate={self.sampling_rate}")
    
    async def should_process_content(self, content: str, content_type: str, priority: str = "normal") -> bool:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥å¤„ç†å†…å®¹ï¼ˆåŠ¨æ€é‡‡æ ·ç­–ç•¥ï¼‰
        
        Args:
            content: å†…å®¹æ–‡æœ¬
            content_type: å†…å®¹ç±»å‹
            priority: ä¼˜å…ˆçº§ (high/normal/low)
        
        Returns:
            bool: æ˜¯å¦åº”è¯¥å¤„ç†
        """
        if not self.enabled:
            return True
        
        # é«˜ä¼˜å…ˆçº§å†…å®¹æ€»æ˜¯å¤„ç†
        if priority == "high":
            return True
        
        # æ£€æŸ¥é¢„ç®—é™åˆ¶
        budget_ok = await self._check_budget()
        if not budget_ok:
            logger.warning("Tokené¢„ç®—å·²è€—å°½ï¼Œè·³è¿‡å¤„ç†")
            return False
        
        # å†…å®¹é•¿åº¦è¿‡çŸ­ï¼Œè·³è¿‡ï¼ˆå¯èƒ½æ˜¯æ— æ„ä¹‰å†…å®¹ï¼‰
        if len(content.strip()) < 5:
            return False
        
        # åŠ¨æ€è®¡ç®—é‡‡æ ·ç‡
        dynamic_rate = self._calculate_dynamic_sampling_rate(content, priority, budget_ok)
        
        # é‡‡æ ·ç­–ç•¥ï¼šæ ¹æ®å†…å®¹å“ˆå¸Œå†³å®šæ˜¯å¦å¤„ç†
        content_hash = hashlib.md5(content.encode()).hexdigest()
        hash_int = int(content_hash[:8], 16)
        
        # ä½¿ç”¨åŠ¨æ€é‡‡æ ·ç‡
        return (hash_int % 100) < (dynamic_rate * 100)
    
    def _calculate_dynamic_sampling_rate(self, content: str, priority: str, budget_ok: bool) -> float:
        """
        åŠ¨æ€è®¡ç®—é‡‡æ ·ç‡
        
        ç­–ç•¥ï¼š
        1. æ ¹æ®é¢„ç®—ä½¿ç”¨ç‡è°ƒæ•´
        2. æ ¹æ®å†…å®¹ç‰¹å¾è°ƒæ•´ï¼ˆé•¿åº¦ã€å…³é”®è¯ï¼‰
        3. æ ¹æ®ä¼˜å…ˆçº§è°ƒæ•´
        """
        base_rate = self.sampling_rate
        
        # è·å–é¢„ç®—ä½¿ç”¨ç‡
        budget_status = self.get_budget_status()
        daily_usage = budget_status.get("daily", {}).get("usage_rate", 0.0)
        hourly_usage = budget_status.get("hourly", {}).get("usage_rate", 0.0)
        max_usage = max(daily_usage, hourly_usage)
        
        # æ ¹æ®é¢„ç®—ä½¿ç”¨ç‡è°ƒæ•´
        if max_usage > 0.8:
            base_rate *= 0.5  # é¢„ç®—ç´§å¼ æ—¶é™ä½é‡‡æ ·ç‡
        elif max_usage > 0.6:
            base_rate *= 0.7
        elif max_usage < 0.3:
            base_rate *= 1.2  # é¢„ç®—å……è¶³æ—¶æé«˜é‡‡æ ·ç‡
        
        # æ ¹æ®å†…å®¹é•¿åº¦è°ƒæ•´ï¼ˆé•¿æ–‡æœ¬æ›´é‡è¦ï¼‰
        content_len = len(content.strip())
        if content_len > 50:
            base_rate *= 1.3
        elif content_len > 100:
            base_rate *= 1.5
        
        # æ ¹æ®ä¼˜å…ˆçº§è°ƒæ•´
        if priority == "low":
            base_rate *= 0.5  # ä½ä¼˜å…ˆçº§é™ä½é‡‡æ ·ç‡
        elif priority == "normal":
            base_rate *= 1.0  # æ­£å¸¸ä¼˜å…ˆçº§ä¿æŒ
        
        # æ£€æŸ¥å…³é”®è¯ï¼ˆé‡è¦å†…å®¹æé«˜é‡‡æ ·ç‡ï¼‰
        important_keywords = ['é‡è¦', 'ç´§æ€¥', 'è¿è§„', 'ä¸¾æŠ¥', 'æ•æ„Ÿ']
        if any(keyword in content for keyword in important_keywords):
            base_rate *= 1.5
        
        # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
        return min(1.0, max(0.1, base_rate))
    
    def optimize_content_for_llm(self, content: str, content_type: str) -> str:
        """
        ä¼˜åŒ–å†…å®¹ä»¥å‡å°‘Tokenæ¶ˆè€—
        
        Args:
            content: åŸå§‹å†…å®¹
            content_type: å†…å®¹ç±»å‹
        
        Returns:
            str: ä¼˜åŒ–åçš„å†…å®¹
        """
        if not self.enabled:
            return content
        
        # 1. å»é™¤å¤šä½™ç©ºç™½å­—ç¬¦
        optimized = ' '.join(content.split())
        
        # 2. é•¿åº¦æˆªæ–­ï¼ˆä¿ç•™é‡è¦ä¿¡æ¯ï¼‰
        if len(optimized) > self.content_max_length:
            # æ™ºèƒ½æˆªæ–­ï¼šä¿ç•™å¼€å¤´å’Œç»“å°¾
            half_length = self.content_max_length // 2 - 10
            optimized = optimized[:half_length] + "..." + optimized[-half_length:]
        
        # 3. ç§»é™¤é‡å¤å­—ç¬¦ï¼ˆå¦‚å¤šä¸ªæ„Ÿå¹å·ï¼‰
        import re
        optimized = re.sub(r'([!?ã€‚ï¼Œ])\1{2,}', r'\1\1', optimized)
        
        # 4. è¡¨æƒ…ç¬¦å·å‹ç¼©
        optimized = re.sub(r'[ğŸ˜€-ğŸ™]{3,}', 'ğŸ˜Š', optimized)
        
        return optimized
    
    def optimize_prompt_for_llm(self, system_prompt: str) -> str:
        """
        ä¼˜åŒ–ç³»ç»ŸPromptä»¥å‡å°‘Tokenæ¶ˆè€—
        
        Args:
            system_prompt: åŸå§‹ç³»ç»ŸPrompt
        
        Returns:
            str: ä¼˜åŒ–åçš„Prompt
        """
        if not self.enabled:
            return system_prompt
        
        # ä½¿ç”¨Promptå‹ç¼©å™¨è¿›è¡Œä¼˜åŒ–
        try:
            from app.services.ai.prompt_compressor import prompt_compressor
            
            # æ ¹æ®é¢„ç®—ä½¿ç”¨ç‡é€‰æ‹©å‹ç¼©ç­–ç•¥
            budget_status = self.get_budget_status()
            daily_usage = budget_status.get("daily", {}).get("usage_rate", 0.0)
            
            if daily_usage > 0.8:
                strategy = "aggressive"  # é¢„ç®—ç´§å¼ æ—¶ä½¿ç”¨æ¿€è¿›å‹ç¼©
            elif daily_usage > 0.5:
                strategy = "moderate"  # é¢„ç®—ä¸­ç­‰æ—¶ä½¿ç”¨ä¸­ç­‰å‹ç¼©
            else:
                strategy = "conservative"  # é¢„ç®—å……è¶³æ—¶ä½¿ç”¨ä¿å®ˆå‹ç¼©
            
            compressed = prompt_compressor.compress_prompt(system_prompt, strategy)
            
            # è®°å½•å‹ç¼©æ•ˆæœ
            savings = prompt_compressor.estimate_token_savings(system_prompt, compressed)
            if savings > 0.1:  # èŠ‚çœè¶…è¿‡10%æ—¶è®°å½•
                logger.debug(f"Promptå‹ç¼©: èŠ‚çœçº¦{savings*100:.1f}% Token")
            
            return compressed
            
        except ImportError:
            # å¦‚æœå‹ç¼©å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸæœ‰ç®€å•ä¼˜åŒ–
            logger.warning("Promptå‹ç¼©å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•ä¼˜åŒ–")
            return self._simple_prompt_optimize(system_prompt)
        except Exception as e:
            logger.error(f"Promptä¼˜åŒ–å¤±è´¥: {e}", exc_info=True)
            return system_prompt
    
    def _simple_prompt_optimize(self, system_prompt: str) -> str:
        """ç®€å•çš„Promptä¼˜åŒ–ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
        import re
        optimized = system_prompt
        
        # å‹ç¼©JSONæ ¼å¼è¦æ±‚
        json_format = '{"score":0-100,"category":"ç±»åˆ«","label":"æ ‡ç­¾","reason":"ç®€çŸ­åŸå› ","confidence":0.0-1.0}'
        
        # æ›¿æ¢å†—é•¿çš„JSONæ ¼å¼è¯´æ˜
        optimized = re.sub(
            r'è¿”å›æ ¼å¼.*?```json.*?```',
            f'è¿”å›JSONæ ¼å¼: {json_format}',
            optimized,
            flags=re.DOTALL
        )
        
        return optimized
    
    def optimize_llm_response(self, response: Dict[str, Any]) -> Dict[str, Any]:
        """
        ä¼˜åŒ–LLMå“åº”ä»¥å‡å°‘å­˜å‚¨å’Œä¼ è¾“æˆæœ¬
        
        Args:
            response: åŸå§‹å“åº”
        
        Returns:
            Dict: ä¼˜åŒ–åçš„å“åº”
        """
        if not self.enabled:
            return response
        
        optimized = response.copy()
        
        # æˆªæ–­reasonå­—æ®µ
        if 'reason' in optimized and len(optimized['reason']) > self.reason_max_length:
            optimized['reason'] = optimized['reason'][:self.reason_max_length] + "..."
        
        # ç§»é™¤ä¸å¿…è¦çš„å­—æ®µ
        unnecessary_fields = ['raw_response', 'debug_info', 'model_version']
        for field in unnecessary_fields:
            optimized.pop(field, None)
        
        return optimized
    
    async def add_to_batch(self, content: str, content_type: str, callback_id: str) -> bool:
        """
        æ·»åŠ åˆ°æ‰¹å¤„ç†é˜Ÿåˆ—
        
        Args:
            content: å†…å®¹
            content_type: ç±»å‹
            callback_id: å›è°ƒID
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸæ·»åŠ 
        """
        if not self.enabled:
            return False
        
        async with self._batch_lock:
            self._batch_queue.append({
                'content': content,
                'content_type': content_type,
                'callback_id': callback_id,
                'timestamp': datetime.utcnow()
            })
            
            # é˜Ÿåˆ—æ»¡äº†æˆ–è¶…æ—¶ï¼Œè§¦å‘æ‰¹å¤„ç†
            if len(self._batch_queue) >= self.batch_size:
                await self._process_batch()
                return True
        
        return False
    
    async def _process_batch(self):
        """å¤„ç†æ‰¹é‡é˜Ÿåˆ— - å®ç°çœŸæ­£çš„æ‰¹é‡è°ƒç”¨ä¼˜åŒ–"""
        if not self._batch_queue:
            return
        
        batch = self._batch_queue.copy()
        self._batch_queue.clear()
        
        logger.info(f"å¼€å§‹æ‰¹å¤„ç† {len(batch)} ä¸ªå†…å®¹")
        
        # æŒ‰å†…å®¹ç±»å‹åˆ†ç»„ï¼Œå®ç°æ‰¹é‡è°ƒç”¨
        danmakus = [item for item in batch if item['content_type'] == 'danmaku']
        comments = [item for item in batch if item['content_type'] == 'comment']
        
        # æ‰¹é‡å¤„ç†å¼¹å¹•
        if danmakus:
            await self._batch_process_items(danmakus, 'danmaku')
        
        # æ‰¹é‡å¤„ç†è¯„è®º
        if comments:
            await self._batch_process_items(comments, 'comment')
    
    async def _batch_process_items(self, items: List[Dict[str, Any]], content_type: str):
        """
        æ‰¹é‡å¤„ç†åŒä¸€ç±»å‹çš„å†…å®¹
        
        ä¼˜åŒ–ç­–ç•¥ï¼š
        1. åˆå¹¶å¤šä¸ªå†…å®¹ä¸ºä¸€ä¸ªLLMè¯·æ±‚ï¼ˆå¦‚æœAPIæ”¯æŒï¼‰
        2. æˆ–ä½¿ç”¨æ‰¹é‡APIè°ƒç”¨å‡å°‘ç½‘ç»œå¼€é”€
        3. è®°å½•æ‰¹é‡å¤„ç†çš„ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            # ç»Ÿè®¡æ‰¹é‡å¤„ç†ä¿¡æ¯
            total_chars = sum(len(item['content']) for item in items)
            optimized_chars = sum(len(self.optimize_content_for_llm(item['content'], content_type)) for item in items)
            chars_saved = total_chars - optimized_chars
            
            # è®°å½•ç»Ÿè®¡ä¿¡æ¯åˆ°Redis
            stats_key = "token_optimizer:stats"
            await redis_service.async_redis.hincrby(stats_key, "batch_processed", len(items))
            await redis_service.async_redis.hincrby(stats_key, "total_tokens_saved", chars_saved)
            await redis_service.async_redis.hset(stats_key, "last_updated", datetime.utcnow().isoformat())
            
            logger.info(f"æ‰¹é‡å¤„ç† {len(items)} ä¸ª{content_type}ï¼ŒèŠ‚çœå­—ç¬¦: {chars_saved}")
            
            # æ³¨æ„ï¼šå®é™…çš„LLMæ‰¹é‡è°ƒç”¨ç”± llm_service çš„é˜Ÿåˆ—æœºåˆ¶å¤„ç†
            # è¿™é‡Œä¸»è¦è´Ÿè´£ç»Ÿè®¡å’Œä¼˜åŒ–è®°å½•
            
        except Exception as e:
            logger.error(f"æ‰¹é‡å¤„ç†å¤±è´¥: {e}", exc_info=True)
    
    async def _check_budget(self) -> bool:
        """æ£€æŸ¥Tokené¢„ç®—æ˜¯å¦å……è¶³"""
        try:
            now = datetime.utcnow()
            current_hour = now.hour
            current_day = now.day
            
            # é‡ç½®å°æ—¶è®¡æ•°
            if current_hour != self.budget.last_reset_hour:
                self.budget.current_hourly = 0
                self.budget.last_reset_hour = current_hour
            
            # é‡ç½®æ—¥è®¡æ•°
            if current_day != self.budget.last_reset_day:
                self.budget.current_daily = 0
                self.budget.last_reset_day = current_day
            
            # ä»Redisè·å–å®é™…ä½¿ç”¨é‡
            daily_key = f"token_budget:daily:{now.strftime('%Y%m%d')}"
            hourly_key = f"token_budget:hourly:{now.strftime('%Y%m%d%H')}"
            
            daily_used = await redis_service.async_redis.get(daily_key) or 0
            hourly_used = await redis_service.async_redis.get(hourly_key) or 0
            
            self.budget.current_daily = int(daily_used)
            self.budget.current_hourly = int(hourly_used)
            
            # æ£€æŸ¥æ˜¯å¦è¶…å‡ºé™åˆ¶
            if self.budget.current_daily >= self.budget.daily_limit:
                return False
            
            if self.budget.current_hourly >= self.budget.hourly_limit:
                return False
            
            return True
            
        except Exception as e:
            logger.error(f"æ£€æŸ¥Tokené¢„ç®—å¤±è´¥: {e}")
            return True  # å‡ºé”™æ—¶å…è®¸å¤„ç†ï¼Œé¿å…é˜»å¡
    
    async def record_token_usage(self, tokens_used: int):
        """
        è®°å½•äº‘ç«¯è°ƒç”¨æˆæœ¬åŸ‹ç‚¹ï¼š
        - token_budget:* æŒ‰â€œæ¬¡æ•°â€è®¡æ•°ï¼ˆç”¨äºé¢„ç®—/é‡‡æ ·ç­–ç•¥ï¼Œä¸ CLOUD_*_BUDGET_CALLS åŒ¹é…ï¼‰
        - token_usage_est:* æŒ‰â€œä¼°ç®— tokenâ€ç´¯åŠ ï¼ˆä»…ç”¨äºæµ‹è¯•/è¯„ä¼°å¯¹æ¯”ï¼‰
        """
        try:
            now = datetime.utcnow()
            day = now.strftime("%Y%m%d")
            hour = now.strftime("%Y%m%d%H")

            call_daily_key = f"token_budget:daily:{day}"
            call_hourly_key = f"token_budget:hourly:{hour}"
            token_daily_key = f"token_usage_est:daily:{day}"
            token_hourly_key = f"token_usage_est:hourly:{hour}"

            est_tokens = int(tokens_used or 0)
            if est_tokens < 0:
                est_tokens = 0
            
            # 1) æ¬¡æ•°ç»Ÿè®¡
            await redis_service.async_redis.incr(call_daily_key)
            await redis_service.async_redis.incr(call_hourly_key)

            # 2) token ä¼°ç®—ï¼ˆå¯é€‰ï¼‰
            if est_tokens:
                await redis_service.async_redis.incrby(token_daily_key, est_tokens)
                await redis_service.async_redis.incrby(token_hourly_key, est_tokens)
            
            # è®¾ç½®è¿‡æœŸæ—¶é—´
            await redis_service.async_redis.expire(call_daily_key, 86400)  # 24å°æ—¶
            await redis_service.async_redis.expire(call_hourly_key, 3600)   # 1å°æ—¶
            await redis_service.async_redis.expire(token_daily_key, 86400)
            await redis_service.async_redis.expire(token_hourly_key, 3600)
            
            # æ›´æ–°å†…å­˜è®¡æ•°ï¼ˆæŒ‰æ¬¡æ•°ï¼Œä¸ *_BUDGET_CALLS åŒ¹é…ï¼‰
            self.budget.current_daily += 1
            self.budget.current_hourly += 1
            
        except Exception as e:
            logger.error(f"è®°å½•Tokenä½¿ç”¨é‡å¤±è´¥: {e}")
    
    def get_budget_status(self) -> Dict[str, Any]:
        """è·å–é¢„ç®—çŠ¶æ€"""
        return {
            "daily": {
                "used": self.budget.current_daily,
                "limit": self.budget.daily_limit,
                "remaining": max(0, self.budget.daily_limit - self.budget.current_daily),
                "usage_rate": self.budget.current_daily / self.budget.daily_limit if self.budget.daily_limit > 0 else 0
            },
            "hourly": {
                "used": self.budget.current_hourly,
                "limit": self.budget.hourly_limit,
                "remaining": max(0, self.budget.hourly_limit - self.budget.current_hourly),
                "usage_rate": self.budget.current_hourly / self.budget.hourly_limit if self.budget.hourly_limit > 0 else 0
            },
            "enabled": self.enabled,
            "sampling_rate": self.sampling_rate
        }
    
    async def get_optimization_stats(self) -> Dict[str, Any]:
        """è·å–ä¼˜åŒ–ç»Ÿè®¡ä¿¡æ¯"""
        try:
            # ä»Redisè·å–ç»Ÿè®¡æ•°æ®
            stats_key = "token_optimizer:stats"
            stats_data = await redis_service.async_redis.hgetall(stats_key)
            
            # è·å–é¢„ç®—çŠ¶æ€
            budget_status = self.get_budget_status()
            
            return {
                "total_processed": int(stats_data.get("total_processed", 0)),
                "total_skipped": int(stats_data.get("total_skipped", 0)),
                "total_tokens_saved": int(stats_data.get("total_tokens_saved", 0)),
                "batch_processed": int(stats_data.get("batch_processed", 0)),
                "average_content_reduction": float(stats_data.get("avg_reduction", 0.0)),
                "last_updated": stats_data.get("last_updated", "æœªçŸ¥"),
                "budget_status": budget_status,
                "sampling_rate": self.sampling_rate,
                "enabled": self.enabled
            }
        except Exception as e:
            logger.error(f"è·å–ä¼˜åŒ–ç»Ÿè®¡å¤±è´¥: {e}")
            return {}
    
    async def record_processed(self, skipped: bool = False):
        """è®°å½•å¤„ç†ç»Ÿè®¡"""
        try:
            stats_key = "token_optimizer:stats"
            if skipped:
                await redis_service.async_redis.hincrby(stats_key, "total_skipped", 1)
            else:
                await redis_service.async_redis.hincrby(stats_key, "total_processed", 1)
        except Exception as e:
            logger.error(f"è®°å½•å¤„ç†ç»Ÿè®¡å¤±è´¥: {e}")


# å…¨å±€å®ä¾‹
token_optimizer = TokenOptimizer()
