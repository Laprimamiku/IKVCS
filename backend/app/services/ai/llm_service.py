"""
LLM æ™ºèƒ½åˆ†ææœåŠ¡ - å‡çº§ç‰ˆ
æ”¯æŒ APO (Automatic Prompt Optimization) + æ¨¡å‹æ³¨å†Œè¡¨ + ç­–ç•¥ç¼–æ’

åŠŸèƒ½ï¼š
1. å¼¹å¹• / è¯„è®ºæ™ºèƒ½è¯„åˆ†ä¸åˆ†ç±»
2. è§„åˆ™é¢„è¿‡æ»¤ï¼Œå‡å°‘ LLM Token æ¶ˆè€—
3. åŠ¨æ€ Promptï¼ˆä»æ•°æ®åº“è¯»å–æ¿€æ´»ç‰ˆæœ¬ï¼‰
4. ç»“æœç»“æ„åŒ–è§£æ
5. Redis ç²¾ç¡®ç¼“å­˜ï¼ˆMD5å“ˆå¸Œï¼‰
6. äº‘ç«¯å¤§æ¨¡å‹ + æœ¬åœ°å°æ¨¡å‹ååŒï¼ˆé€šè¿‡æ¨¡å‹æ³¨å†Œè¡¨ï¼‰
7. å›¾åƒè¯†åˆ«åˆ†æï¼ˆäº‘ç«¯æ¨¡å‹ï¼‰
8. å¼‚æ­¥åå°ä»»åŠ¡ï¼Œç›´æ¥æ›´æ–°æ•°æ®åº“
9. æˆæœ¬æ§åˆ¶ä¸é¢„ç®—ç®¡ç†

æ¶æ„ï¼š
- Layer 1: è§„åˆ™è¿‡æ»¤
- Layer 1.5: ç²¾ç¡®ç¼“å­˜ï¼ˆRedisï¼‰
- Layer 1.6: è¯­ä¹‰ç¼“å­˜ï¼ˆå¯é€‰ï¼‰
- Layer 2: æœ¬åœ°/äº‘ç«¯æ¨¡å‹ï¼ˆé€šè¿‡æ¨¡å‹æ³¨å†Œè¡¨ï¼‰
- Layer 3: å¤šæ™ºèƒ½ä½“é™ªå®¡å›¢ï¼ˆå¯é€‰ï¼‰
"""

import asyncio
import httpx
import json
import logging
import hashlib
import random
import re
from dataclasses import dataclass
from typing import Optional, Dict, Any, List, Tuple, Set
from datetime import datetime

from app.core.config import settings
from app.core.types import AIContentAnalysisResult
from app.core.database import SessionLocal
from app.core.model_registry import model_registry
from app.models.danmaku import Danmaku
from app.models.comment import Comment
from app.models.ai_prompt_version import AiPromptVersion
from app.services.ai.prompts import (
    DANMAKU_SYSTEM_PROMPT,
    COMMENT_SYSTEM_PROMPT,
    DANMAKU_SYSTEM_PROMPT_LOCAL,
    COMMENT_SYSTEM_PROMPT_LOCAL,
    DANMAKU_SYSTEM_PROMPT_CLOUD,
    COMMENT_SYSTEM_PROMPT_CLOUD
)
from app.services.cache.redis_service import redis_service
from app.utils.timezone_utils import isoformat_in_app_tz, utc_now
from app.services.ai.embedding_service import embedding_service  # å…è®¸è¿”å› Noneï¼Œä¸å½±å“ä¸»æµç¨‹
from app.services.ai.local_model_service import local_model_service
from app.services.ai.token_optimizer import token_optimizer

logger = logging.getLogger(__name__)


@dataclass(frozen=True)
class _AnalysisQueueItem:
    content_type: str  # "danmaku" | "comment"
    item_id: int
    priority: str = "low"

# ==================== å¤šæ™ºèƒ½ä½“å»¶è¿ŸåŠ è½½ ====================

_multi_agent_service = None


def _get_multi_agent_service():
    global _multi_agent_service
    if _multi_agent_service is None:
        try:
            from app.services.ai.multi_agent_service import multi_agent_service
            _multi_agent_service = multi_agent_service
        except ImportError as e:
            logger.debug(f"å¤šæ™ºèƒ½ä½“æœåŠ¡ä¸å¯ç”¨: {e}")
            _multi_agent_service = None
    return _multi_agent_service


# ==================== LLM Service ====================

class LLMService:
    def __init__(self):
        # è¿è¡Œæ¨¡å¼
        self.mode = getattr(settings, "LLM_MODE", "hybrid").lower()
        
        # æˆæœ¬æ§åˆ¶
        self.cost_tracker = {}  # video_id -> {"calls": count, "chars": count}
        
        self.default_response: AIContentAnalysisResult = {
            "score": 60,
            "category": "æ™®é€š",
            "label": "æ™®é€š",
            "reason": "é»˜è®¤å¤„ç†",
            "is_highlight": False,
            "is_inappropriate": False,
            "confidence": 0.5,
            "source": "default",
            "model_name": "default",
            "prompt_version_id": None,
            "decision_trace": []
        }

        # é«˜é¢‘çŸ­æ–‡æœ¬ï¼šé˜Ÿåˆ—æ‰¹å¤„ç†ï¼ˆé™ä½äº‘ç«¯ token + é¿å… GPU/IO å³°å€¼ï¼‰
        def _as_bool(value: object, default: bool = False) -> bool:
            if isinstance(value, bool):
                return value
            if isinstance(value, (int, float)) and not isinstance(value, bool):
                return bool(value)
            if isinstance(value, str):
                v = value.strip().lower()
                if v in {"1", "true", "t", "yes", "y", "on"}:
                    return True
                if v in {"0", "false", "f", "no", "n", "off"}:
                    return False
            return default

        def _as_int(value: object, default: int) -> int:
            if isinstance(value, bool):
                return default
            if isinstance(value, int):
                return value
            if isinstance(value, float):
                return int(value)
            if isinstance(value, str):
                v = value.strip()
                if not v:
                    return default
                try:
                    return int(float(v))
                except (TypeError, ValueError):
                    return default
            return default

        def _as_float(value: object, default: float) -> float:
            if isinstance(value, bool):
                return default
            if isinstance(value, (int, float)):
                return float(value)
            if isinstance(value, str):
                v = value.strip()
                if not v:
                    return default
                try:
                    return float(v)
                except (TypeError, ValueError):
                    return default
            return default

        self.queue_enabled: bool = _as_bool(getattr(settings, "AI_ANALYSIS_QUEUE_ENABLED", False), False)
        self._analysis_queue_maxsize: int = _as_int(getattr(settings, "AI_ANALYSIS_QUEUE_MAXSIZE", 0), 0)
        self._analysis_queue: asyncio.Queue[_AnalysisQueueItem] = asyncio.Queue(maxsize=max(0, self._analysis_queue_maxsize))
        self._analysis_pending: Set[Tuple[str, int]] = set()
        self._analysis_workers: List[asyncio.Task] = []
        self._analysis_batch_size: int = max(1, _as_int(getattr(settings, "AI_ANALYSIS_BATCH_SIZE", 30), 30))
        batch_window_ms = _as_float(getattr(settings, "AI_ANALYSIS_BATCH_WINDOW_MS", 500), 500.0)
        self._analysis_batch_window_s: float = max(0.05, batch_window_ms / 1000.0)
        self._analysis_worker_count: int = max(1, _as_int(getattr(settings, "AI_ANALYSIS_QUEUE_WORKERS", 1), 1))
        
        # è®°å½•å½“å‰é…ç½®
        logger.info(f"LLM Service åˆå§‹åŒ–å®Œæˆ:")
        logger.info(f"  LLM_MODE: {self.mode}")
        logger.info(f"  AI_ANALYSIS_QUEUE_ENABLED: {self.queue_enabled}")
        
        if self.mode != "off":
            available_models = model_registry.get_available_models()
            for model_type, config in available_models.items():
                logger.info(f"  {model_type}: {config.name} @ {config.base_url}")
        else:
            logger.info("  LLM_MODE=offï¼Œæ‰€æœ‰AIåŠŸèƒ½å·²ç¦ç”¨")

    # ==================== å·¥å…·æ–¹æ³• ====================

    def _get_content_hash(self, content: str, prompt_version_id: Optional[int] = None) -> str:
        """ç”Ÿæˆå†…å®¹å“ˆå¸Œï¼ŒåŒ…å« Prompt ç‰ˆæœ¬ä¿¡æ¯"""
        hash_input = content.strip()
        if prompt_version_id:
            hash_input += f"_v{prompt_version_id}"
        return hashlib.md5(hash_input.encode("utf-8")).hexdigest()

    def _normalize_for_dedup(self, content: str) -> str:
        """çŸ­æ–‡æœ¬å»é‡ç”¨çš„è½»é‡å½’ä¸€åŒ–ï¼ˆæ¯” embedding ä¾¿å®œï¼‰"""
        text = " ".join((content or "").split()).strip().lower()
        # åˆå¹¶é‡å¤æ ‡ç‚¹/è¡¨æƒ…ç¬¦å·ï¼Œé¿å…â€œå“ˆå“ˆå“ˆå“ˆâ€â€œ!!!!!â€é€ æˆå»é‡å¤±è´¥
        text = re.sub(r"([!ï¼?ï¼Ÿã€‚ï¼Œ,.â€¦~ï½])\\1{1,}", r"\\1", text)
        text = re.sub(r"(å“ˆ)\\1{2,}", r"\\1\\1", text)
        return text[:300]

    def _semantic_threshold_for(self, content: str) -> float:
        """çŸ­æ–‡æœ¬æ›´ä¿å®ˆï¼Œé¿å…è¯­ä¹‰ç¼“å­˜è¯¯å‘½ä¸­"""
        base = getattr(settings, "AI_SEMANTIC_CACHE_THRESHOLD", 0.95)
        short = getattr(settings, "AI_SEMANTIC_CACHE_THRESHOLD_SHORT", base)
        return short if len(content) < 30 else base

    def _should_store_trace(self, result: AIContentAnalysisResult) -> bool:
        mode = (getattr(settings, "AI_ANALYSIS_TRACE_MODE", "risky") or "risky").lower()
        if mode == "none":
            return False
        if mode == "all":
            return True
        if mode == "sample":
            rate = float(getattr(settings, "AI_ANALYSIS_TRACE_SAMPLE_RATE", 0.05) or 0.0)
            return random.random() < max(0.0, min(1.0, rate))

        # riskyï¼ˆé»˜è®¤ï¼‰ï¼šä»…å¼‚å¸¸/ä½ç½®ä¿¡/ä½åˆ†å†™å…¥ï¼Œé¿å… DB/IO å†™æ”¾å¤§
        risk_score = int(getattr(settings, "AI_ANALYSIS_TRACE_RISK_SCORE", 55) or 55)
        low_conf = float(getattr(settings, "AI_ANALYSIS_TRACE_LOW_CONFIDENCE", 0.6) or 0.6)
        score = int(result.get("score", 60) or 60)
        conf = float(result.get("confidence", 0.5) or 0.5)
        if bool(result.get("is_inappropriate")):
            return True
        return score < risk_score or conf < low_conf

    async def start_analysis_queue(self) -> None:
        if not self.queue_enabled:
            return
        if self._analysis_workers:
            return
        for i in range(self._analysis_worker_count):
            self._analysis_workers.append(asyncio.create_task(self._analysis_worker_loop(i)))
        logger.info(
            "[AIQueue] started workers=%s batch_size=%s window_ms=%s maxsize=%s",
            self._analysis_worker_count,
            self._analysis_batch_size,
            int(self._analysis_batch_window_s * 1000),
            self._analysis_queue_maxsize,
        )

    async def stop_analysis_queue(self) -> None:
        if not self._analysis_workers:
            return
        for t in self._analysis_workers:
            t.cancel()
        await asyncio.gather(*self._analysis_workers, return_exceptions=True)
        self._analysis_workers.clear()
        logger.info("[AIQueue] stopped")

    async def enqueue_analysis(self, content_type: str, item_id: int, priority: str = "low") -> None:
        if not self.queue_enabled:
            return
        key = (content_type, int(item_id))
        if key in self._analysis_pending:
            return
        self._analysis_pending.add(key)
        try:
            self._analysis_queue.put_nowait(
                _AnalysisQueueItem(content_type=content_type, item_id=int(item_id), priority=priority)
            )
            logger.info(
                "[AIQueue] enqueued %s:%s priority=%s qsize=%s",
                content_type,
                item_id,
                priority,
                self._analysis_queue.qsize(),
            )
        except asyncio.QueueFull:
            # ä¸é˜»å¡ä¸»æµç¨‹ï¼šé˜Ÿåˆ—æ»¡æ—¶ç›´æ¥ä¸¢å¼ƒï¼Œä¾èµ–ç¼“å­˜/è§„åˆ™å…œåº•
            self._analysis_pending.discard(key)
            logger.warning("[AIQueue] queue_full drop %s:%s", content_type, item_id)

    async def _drain_batch(self) -> List[_AnalysisQueueItem]:
        first = await self._analysis_queue.get()
        self._analysis_pending.discard((first.content_type, first.item_id))
        batch: List[_AnalysisQueueItem] = [first]

        loop = asyncio.get_running_loop()
        start = loop.time()
        while len(batch) < self._analysis_batch_size:
            remaining = self._analysis_batch_window_s - (loop.time() - start)
            if remaining <= 0:
                break
            try:
                item = await asyncio.wait_for(self._analysis_queue.get(), timeout=remaining)
            except asyncio.TimeoutError:
                break
            self._analysis_pending.discard((item.content_type, item.item_id))
            batch.append(item)

        return batch

    async def _analysis_worker_loop(self, worker_id: int) -> None:
        while True:
            batch = await self._drain_batch()
            try:
                await self._process_analysis_batch(batch, worker_id=worker_id)
            except Exception as e:
                logger.error("[AIQueue] worker=%s batch_process_failed: %s", worker_id, e, exc_info=True)

    async def _process_analysis_batch(self, batch: List[_AnalysisQueueItem], worker_id: int = 0) -> None:
        # åˆ†ç±»å‹èšåˆ
        comment_ids = [t.item_id for t in batch if t.content_type == "comment"]
        danmaku_ids = [t.item_id for t in batch if t.content_type == "danmaku"]

        if not comment_ids and not danmaku_ids:
            return

        db = SessionLocal()
        try:
            comments = []
            danmakus = []
            if comment_ids:
                comments = db.query(Comment).filter(Comment.id.in_(comment_ids)).all()
            if danmaku_ids:
                danmakus = db.query(Danmaku).filter(Danmaku.id.in_(danmaku_ids)).all()

            # group by normalized content to analyze once
            groups: Dict[Tuple[str, str], List[Any]] = {}
            priorities: Dict[Tuple[str, str], str] = {}

            id_to_priority: Dict[Tuple[str, int], str] = {(t.content_type, t.item_id): t.priority for t in batch}

            for c in comments:
                key = ("comment", self._normalize_for_dedup(c.content))
                groups.setdefault(key, []).append(c)
                priorities.setdefault(key, id_to_priority.get(("comment", c.id), "low"))

            for d in danmakus:
                key = ("danmaku", self._normalize_for_dedup(d.content))
                groups.setdefault(key, []).append(d)
                priorities.setdefault(key, id_to_priority.get(("danmaku", d.id), "low"))

            processed = 0
            for (content_type, _norm), items in groups.items():
                # ä½¿ç”¨é¦–æ¡å†…å®¹ä½œä¸ºä»£è¡¨
                content = (items[0].content or "").strip()
                if not content:
                    continue
                priority = priorities.get((content_type, _norm), "low")
                priority = self._effective_priority(content_type, content, priority)
                result = await self.analyze_content_with_policy(
                    content=content,
                    content_type=content_type,
                    force_jury=False,
                    video_id=None,
                    priority=priority,
                )
                processed += len(items)

                trace = result.get("decision_trace")
                store_trace = bool(trace) and self._should_store_trace(result)
                trace_json = None
                if store_trace:
                    try:
                        trace_json = json.dumps(trace, ensure_ascii=False)
                    except Exception:
                        trace_json = None

                for obj in items:
                    if content_type == "danmaku":
                        obj.ai_score = result.get("score", 60)
                        obj.ai_category = result.get("category", "æ™®é€š")
                        obj.ai_reason = result.get("reason")
                        obj.ai_confidence = result.get("confidence", 0.5)
                        obj.ai_source = result.get("source")
                        obj.ai_prompt_version_id = result.get("prompt_version_id")
                        obj.ai_model = result.get("model_name")
                        if store_trace:
                            obj.ai_trace = trace_json
                        obj.is_highlight = bool(result.get("is_highlight", False) or (obj.ai_score or 0) >= 90)
                    else:
                        obj.ai_score = result.get("score", 60)
                        obj.ai_label = result.get("label", "æ™®é€š")
                        obj.ai_reason = result.get("reason")
                        conf = result.get("confidence", 0.5)
                        obj.ai_confidence = int(float(conf) * 100) if conf is not None else None
                        obj.ai_source = result.get("source")
                        obj.ai_prompt_version_id = result.get("prompt_version_id")
                        obj.ai_model = result.get("model_name")
                        if store_trace:
                            obj.ai_trace = trace_json

            if processed:
                db.commit()
                logger.info("[AIQueue] worker=%s processed=%s batch=%s", worker_id, processed, len(batch))

        except Exception:
            db.rollback()
            raise
        finally:
            db.close()

    async def _mark_metric(self, metric: str) -> None:
        """è½»é‡åŸ‹ç‚¹ï¼šè®°å½•è§„åˆ™/ç¼“å­˜/äº‘ç«¯/æœ¬åœ°/jury è°ƒç”¨æ¬¡æ•°ã€‚"""
        try:
            await redis_service.incr_metric(metric)
        except Exception:
            # åŸ‹ç‚¹å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
            pass

    def _check_budget(self, video_id: Optional[int], input_chars: int) -> bool:
        """æ£€æŸ¥é¢„ç®—é™åˆ¶"""
        if not video_id:
            return True
            
        # è·å–é¢„ç®—é™åˆ¶é…ç½®
        max_calls = getattr(settings, 'CLOUD_MAX_CALLS_PER_VIDEO', 0)
        max_chars = getattr(settings, 'CLOUD_MAX_INPUT_CHARS_PER_VIDEO', 0)
        
        if max_calls <= 0 and max_chars <= 0:
            return True  # æ— é™åˆ¶
        
        current = self.cost_tracker.get(video_id, {"calls": 0, "chars": 0})
        
        # æ£€æŸ¥è°ƒç”¨æ¬¡æ•°é™åˆ¶
        if max_calls > 0 and current["calls"] >= max_calls:
            logger.warning(f"è§†é¢‘ {video_id} å·²è¾¾åˆ°äº‘ç«¯è°ƒç”¨æ¬¡æ•°é™åˆ¶: {current['calls']}")
            return False
        
        # æ£€æŸ¥å­—ç¬¦æ•°é™åˆ¶
        if max_chars > 0 and current["chars"] + input_chars > max_chars:
            logger.warning(f"è§†é¢‘ {video_id} å·²è¾¾åˆ°äº‘ç«¯å­—ç¬¦æ•°é™åˆ¶: {current['chars']} + {input_chars}")
            return False
        
        return True

    def _update_budget(self, video_id: Optional[int], input_chars: int):
        """æ›´æ–°é¢„ç®—ä½¿ç”¨æƒ…å†µ"""
        if not video_id:
            return
        
        if video_id not in self.cost_tracker:
            self.cost_tracker[video_id] = {"calls": 0, "chars": 0}
        
        self.cost_tracker[video_id]["calls"] += 1
        self.cost_tracker[video_id]["chars"] += input_chars

    # ==================== Prompt ç‰ˆæœ¬ç®¡ç†ï¼ˆå·²æ³¨é‡Šï¼Œä¸å†ä½¿ç”¨ï¼‰ ====================
    # def _get_active_prompt(self, content_type: str) -> tuple[str, Optional[int]]:
    #     """è·å–æ¿€æ´»çš„ Prompt ç‰ˆæœ¬"""
    #     db = SessionLocal()
    #     try:
    #         # æ˜ å°„å†…å®¹ç±»å‹åˆ° Prompt ç±»å‹
    #         prompt_type_map = {
    #             "danmaku": "DANMAKU",
    #             "comment": "COMMENT"
    #         }
    #         prompt_type = prompt_type_map.get(content_type)
    #         
    #         if prompt_type:
    #             # æŸ¥è¯¢æ¿€æ´»ç‰ˆæœ¬
    #             active_version = db.query(AiPromptVersion).filter(
    #                 AiPromptVersion.prompt_type == prompt_type,
    #                 AiPromptVersion.is_active == True
    #             ).first()
    #             
    #             if active_version:
    #                 logger.debug(f"ä½¿ç”¨æ•°æ®åº“ Prompt ç‰ˆæœ¬ {active_version.id}: {prompt_type}")
    #                 return active_version.prompt_content, active_version.id
    #         
    #         # å›é€€åˆ°ç¡¬ç¼–ç  Prompt
    #         fallback_prompt = (
    #             DANMAKU_SYSTEM_PROMPT if content_type == "danmaku" 
    #             else COMMENT_SYSTEM_PROMPT
    #         )
    #         logger.debug(f"ä½¿ç”¨ç¡¬ç¼–ç  Prompt: {content_type}")
    #         return fallback_prompt, None
    #         
    #     except Exception as e:
    #         logger.warning(f"è·å– Prompt ç‰ˆæœ¬å¤±è´¥: {e}")
    #         # å›é€€åˆ°ç¡¬ç¼–ç  Prompt
    #         fallback_prompt = (
    #             DANMAKU_SYSTEM_PROMPT if content_type == "danmaku" 
    #             else COMMENT_SYSTEM_PROMPT
    #         )
    #         return fallback_prompt, None
    #     finally:
    #         db.close()
    
    def _get_prompt_for_model(self, content_type: str, model_type: str) -> str:
        """æ ¹æ®æ¨¡å‹ç±»å‹è·å–å¯¹åº”çš„æç¤ºè¯"""
        if model_type == "local_text":
            # æœ¬åœ°æ¨¡å‹ä½¿ç”¨ç®€åŒ–æç¤ºè¯
            if content_type == "danmaku":
                return DANMAKU_SYSTEM_PROMPT_LOCAL
            else:
                return COMMENT_SYSTEM_PROMPT_LOCAL
        else:
            # äº‘ç«¯æ¨¡å‹ä½¿ç”¨å®Œæ•´æç¤ºè¯
            if content_type == "danmaku":
                return DANMAKU_SYSTEM_PROMPT_CLOUD
            else:
                return COMMENT_SYSTEM_PROMPT_CLOUD

    # ==================== æ ¸å¿ƒåˆ†ææµç¨‹ ====================

    async def analyze_content_with_policy(
        self, 
        content: str, 
        content_type: str, 
        force_jury: bool = False,
        video_id: Optional[int] = None,
        priority: str = "normal"
    ) -> AIContentAnalysisResult:
        """
        ç­–ç•¥ç¼–æ’çš„æ™ºèƒ½å†…å®¹åˆ†æï¼ˆAPO + Multi-Agent + Tokenä¼˜åŒ–ï¼‰
        
        Layer 1   : è§„åˆ™è¿‡æ»¤
        Layer 1.5 : ç²¾ç¡®ç¼“å­˜ï¼ˆMD5ï¼‰
        Layer 1.6 : è¯­ä¹‰ç¼“å­˜ï¼ˆå¯é€‰ï¼‰
        Layer 2   : æœ¬åœ°/äº‘ç«¯æ¨¡å‹ï¼ˆé€šè¿‡æ¨¡å‹æ³¨å†Œè¡¨ï¼‰
        Layer 3   : å¤šæ™ºèƒ½ä½“é™ªå®¡å›¢ï¼ˆå¯é€‰ï¼‰
        """
        if not content:
            return self.default_response.copy()
        
        # ä¼˜åŒ–å†…å®¹ä»¥å‡å°‘Tokenæ¶ˆè€—
        optimized_content = token_optimizer.optimize_content_for_llm(content, content_type)
        
        # ä¸å†ä½¿ç”¨Promptç‰ˆæœ¬ç®¡ç†ï¼Œprompt_version_idå›ºå®šä¸ºNone
        prompt_version_id = None
        
        trace = [
            {
                "step": "start", 
                "mode": self.mode, 
                "prompt_version_id": None,
                "content_optimized": len(optimized_content) < len(content),
                "timestamp": isoformat_in_app_tz(utc_now()),
            }
        ]

        # ==================== Layer 1: è§„åˆ™è¿‡æ»¤ ====================
        pre_check = self._rule_based_filter(optimized_content, content_type)
        if pre_check:
            await self._mark_metric("rule_hit")
            pre_check["prompt_version_id"] = prompt_version_id
            pre_check["decision_trace"] = trace + [{"step": "rule_hit"}]
            # ä¼˜åŒ–è¾“å‡º
            return token_optimizer.optimize_llm_response(pre_check)

        # ==================== Layer 1.5: ç²¾ç¡®ç¼“å­˜ ====================
        content_hash = self._get_content_hash(optimized_content, prompt_version_id)
        exact_cache_key = f"ai:analysis:{content_type}:{content_hash}"

        try:
            cached = await redis_service.async_redis.get(exact_cache_key)
            if cached:
                logger.info(f"AI Exact Cache Hit: {optimized_content[:10]}...")
                result = json.loads(cached)
                result.setdefault("source", "cache_exact")
                result["prompt_version_id"] = prompt_version_id
                result["decision_trace"] = trace + [{"step": "cache_exact"}]
                await self._mark_metric("exact_hit")
                # å¤šæ™ºèƒ½ä½“é™ªå®¡å›¢å·²æ³¨é‡Šï¼Œç›´æ¥è¿”å›ç»“æœ
                return result
        except Exception as e:
            logger.warning(f"Exact cache read failed: {e}")

        # ==================== Layer 1.6: è¯­ä¹‰ç¼“å­˜ï¼ˆåˆ†å±‚ç¼“å­˜ç­–ç•¥ï¼‰ ====================
        sem_result = None
        embedding = None
        try:
            min_len = int(getattr(settings, "AI_SEMANTIC_CACHE_MIN_LEN", 0) or 0)
            if len(optimized_content) < min_len:
                embedding = None
            else:
                embedding = await embedding_service.get_text_embedding(optimized_content)
            if embedding:
                sem_key_prefix = f"ai:semcache:{content_type}"
                
                # å¼ºåŒ–è¯­ä¹‰ç¼“å­˜ï¼šåˆ†å±‚ç¼“å­˜ç­–ç•¥ï¼Œé™ä½é˜ˆå€¼ä»¥è¯†åˆ«æ›´å¤šå«ä¹‰é«˜åº¦ç›¸ä¼¼çš„è¯­å¥
                # åŸºç¡€é˜ˆå€¼å·²ä»0.95é™ä½åˆ°0.88ï¼Œè¿™é‡Œè¿›ä¸€æ­¥é™ä½ä»¥å¢å¼ºç›¸ä¼¼è¯­å¥è¯†åˆ«
                base_threshold = self._semantic_threshold_for(optimized_content)
                thresholds = [
                    base_threshold,  # åŸºç¡€é˜ˆå€¼ï¼ˆ0.88æˆ–0.85ï¼‰
                    base_threshold - 0.05,  # é™ä½5%ä½œä¸ºç¬¬äºŒå±‚
                    base_threshold - 0.08,  # é™ä½8%ä½œä¸ºç¬¬ä¸‰å±‚
                    base_threshold - 0.10,  # é™ä½10%ä½œä¸ºç¬¬å››å±‚ï¼ˆæ›´å®½æ¾ï¼Œè¯†åˆ«æ›´å¤šç›¸ä¼¼è¯­å¥ï¼‰
                ]
                
                for threshold in thresholds:
                    sem_cached = await redis_service.search_similar_vector(
                        cache_key_prefix=sem_key_prefix,
                        embedding=embedding,
                        threshold=threshold,
                    )
                    if sem_cached:
                        sem_result = json.loads(sem_cached)
                        sem_result.setdefault("source", "cache_semantic")
                        sem_result["prompt_version_id"] = prompt_version_id
                        sem_result["decision_trace"] = trace + [{"step": "cache_semantic", "threshold": threshold}]
                        await self._mark_metric("semantic_hit")
                        logger.info(f"è¯­ä¹‰ç¼“å­˜å‘½ä¸­ (é˜ˆå€¼={threshold:.2f}): {optimized_content[:20]}...")
                        # å¤šæ™ºèƒ½ä½“é™ªå®¡å›¢å·²æ³¨é‡Šï¼Œç›´æ¥è¿”å›ç»“æœ
                        return sem_result
        except Exception as e:
            logger.warning(f"Semantic cache failed: {e}")

        # ==================== Layer 2: æ¨¡å‹æ¨ç†ï¼ˆé€šè¿‡æ¨¡å‹æ³¨å†Œè¡¨ï¼‰ ====================
        text_models = model_registry.get_text_model_priority()
        
        if not text_models:
            logger.debug("LLM_MODE=off æˆ–æœªé…ç½®ä»»ä½•æ¨¡å‹ï¼Œè¿”å›é»˜è®¤ç»“æœ")
            result = self.default_response.copy()
            result["prompt_version_id"] = prompt_version_id
            result["decision_trace"] = trace + [{"step": "llm_off"}]
            await self._mark_metric("llm_off")
            return result

        # å°è¯•æŒ‰ä¼˜å…ˆçº§ä½¿ç”¨æ¨¡å‹
        for model_type in text_models:
            model_config = model_registry.get_model(model_type)
            if not model_config:
                continue
            
            try:
                # é¢„ç®—æ£€æŸ¥å·²æ³¨é‡Šï¼Œä¸å†æ£€æŸ¥é¢„ç®—é™åˆ¶
                # if model_type.startswith("cloud"):
                #     input_chars = len(optimized_content) + len(optimized_prompt)
                #     if not self._check_budget(video_id, input_chars):
                #         logger.warning(f"é¢„ç®—é™åˆ¶ï¼Œè·³è¿‡äº‘ç«¯æ¨¡å‹ {model_type}")
                #         result = self.default_response.copy()
                #         result["prompt_version_id"] = prompt_version_id
                #         result["decision_trace"] = trace + [{"step": "budget_exceeded"}]
                #         result["reason"] = "é¢„ç®—ä¸è¶³ï¼Œéœ€äººå·¥å¤æ ¸"
                #         return result
                
                # è°ƒç”¨æ¨¡å‹
                if model_type == "cloud_text":
                    # äº‘ç«¯è°ƒç”¨ï¼šå¯é€‰é‡‡æ ·/é¢„ç®—æ§åˆ¶ï¼ˆæœ¬åœ°æ¨ç†ä»å¯è·‘ï¼Œäº‘ç«¯åªå¤„ç†â€œä¸ç¡®å®š/é«˜é£é™©â€ï¼‰
                    # æ³¨æ„ï¼šé‡‡æ ·ç­–ç•¥ä»…ç”¨äº hybridï¼ˆæœ¬åœ°ä¼˜å…ˆ + äº‘ç«¯å…œåº•ï¼‰ä»¥é™æœ¬å¢æ•ˆï¼›
                    # cloud_only æ¨¡å¼ä¸‹è‹¥é‡‡æ ·è·³è¿‡ï¼Œä¼šå¯¼è‡´æ²¡æœ‰ä»»ä½•æ¨¡å‹æ¨ç†ï¼Œå½±å“â€œå¼ºåˆ¶äº‘ç«¯â€çš„é¢„æœŸã€‚
                    # é‡‡æ ·ç­–ç•¥å·²æ³¨é‡Šï¼Œä¸å†è·³è¿‡äº‘ç«¯æ¨¡å‹è°ƒç”¨
                    # if self.mode == "hybrid":
                    #     if not await token_optimizer.should_process_content(content, content_type, priority):
                    #         await self._mark_metric("cloud_skip_optimizer")
                    #         continue
                    logger.info(
                        "[AIText] cloud_try type=%s model=%s chars=%s",
                        content_type,
                        getattr(model_config, "name", "unknown"),
                        len(optimized_content),
                    )
                    await self._mark_metric("cloud_attempt")
                    # è·å–äº‘ç«¯æ¨¡å‹ä¸“ç”¨æç¤ºè¯
                    cloud_prompt = self._get_prompt_for_model(content_type, "cloud_text")
                    result = await self._call_cloud_model(optimized_content, cloud_prompt, model_config)
                    if result:
                        # é¢„ç®—æ›´æ–°å·²æ³¨é‡Š
                        # self._update_budget(video_id, input_chars)
                        await self._mark_metric("cloud_call")
                        # è®°å½•Tokenä½¿ç”¨
                        # await token_optimizer.record_token_usage(input_chars // 4)  # ç²—ç•¥ä¼°ç®—tokenæ•°
                elif model_type == "local_text":
                    logger.info(
                        "[AIText] local_try type=%s model=%s chars=%s",
                        content_type,
                        getattr(model_config, "name", "unknown"),
                        len(optimized_content),
                    )
                    # è·å–æœ¬åœ°æ¨¡å‹ä¸“ç”¨æç¤ºè¯
                    local_prompt = self._get_prompt_for_model(content_type, "local_text")
                    result = await self._call_local_model(
                        optimized_content,
                        content_type,
                        local_prompt,
                        model_config
                    )
                    if result:
                        await self._mark_metric("local_call")
                        # å‡çº§é€»è¾‘å·²æ³¨é‡Šï¼Œä¸å†æ£€æŸ¥æ˜¯å¦éœ€è¦å‡çº§åˆ°äº‘ç«¯
                        # confidence = result.get("confidence", 0.5)
                        # min_chars = int(getattr(settings, "LOCAL_LLM_ESCALATE_MIN_CHARS", 0) or 0)
                        # if (len(optimized_content) >= min_chars and
                        #     model_registry.should_escalate_to_cloud(confidence) and
                        #     model_registry.is_available("cloud_text")):
                        #     logger.info(f"æœ¬åœ°æ¨¡å‹ç½®ä¿¡åº¦ä½ ({confidence:.2f})ï¼Œå‡çº§åˆ°äº‘ç«¯æ¨¡å‹")
                        #     cloud_config = model_registry.get_model("cloud_text")
                        #     if cloud_config and self._check_budget(video_id, input_chars):
                        #         # å‡çº§äº‘ç«¯ï¼šä¸èµ°é‡‡æ ·ï¼Œå±äº"éš¾å†…å®¹è¡¥å®¡"è·¯å¾„
                        #         cloud_result = await self._call_cloud_model(optimized_content, optimized_prompt, cloud_config)
                        #         if cloud_result:
                        #             self._update_budget(video_id, input_chars)
                        #             await self._mark_metric("cloud_call")
                        #             await token_optimizer.record_token_usage(input_chars // 4)
                        #             result = cloud_result
                        #             result["decision_trace"] = trace + [
                        #                 {"step": "local_llm", "confidence": confidence},
                        #                 {"step": "escalate_to_cloud"}
                        #             ]
                
                if result:
                    logger.info(
                        "[AIText] done type=%s source=%s model=%s score=%s conf=%s",
                        content_type,
                        result.get("source"),
                        result.get("model_name"),
                        result.get("score"),
                        result.get("confidence"),
                    )
                    result["prompt_version_id"] = prompt_version_id
                    result.setdefault("decision_trace", trace + [{"step": model_type}])
                    
                    # ä¼˜åŒ–è¾“å‡º
                    result = token_optimizer.optimize_llm_response(result)
                    
                    # ä¿å­˜ç¼“å­˜
                    await self._save_cache(optimized_content, content_type, result, embedding, prompt_version_id)
                    
                    # å¤šæ™ºèƒ½ä½“é™ªå®¡å›¢å·²æ³¨é‡Šï¼Œç›´æ¥è¿”å›ç»“æœ
                    return result
                    
            except Exception as e:
                logger.error(f"æ¨¡å‹ {model_type} è°ƒç”¨å¤±è´¥: {e}")
                continue
        
        # æ‰€æœ‰æ¨¡å‹éƒ½å¤±è´¥ï¼Œè¿”å›é»˜è®¤ç»“æœ
        logger.warning("æ‰€æœ‰æ¨¡å‹è°ƒç”¨å¤±è´¥ï¼Œè¿”å›é»˜è®¤ç»“æœ")
        result = self.default_response.copy()
        result["prompt_version_id"] = prompt_version_id
        result["decision_trace"] = trace + [{"step": "all_models_failed"}]
        return result

    # ä¿æŒå‘åå…¼å®¹çš„æ–¹æ³•
    async def analyze_content(
        self,
        content: str,
        content_type: str,
        force_jury: bool = False,
        priority: str = "normal",
        video_id: Optional[int] = None,
    ) -> AIContentAnalysisResult:
        """å‘åå…¼å®¹çš„åˆ†ææ–¹æ³•"""
        # æ‰‹åŠ¨è§¦å‘/å¤æ ¸å±äºæ˜ç¡®çš„äººå·¥æ“ä½œï¼šå¿…é¡»è·³è¿‡é‡‡æ ·è·³è¿‡é€»è¾‘
        effective_priority = "high" if force_jury else priority
        return await self.analyze_content_with_policy(
            content=content,
            content_type=content_type,
            force_jury=force_jury,
            video_id=video_id,
            priority=effective_priority,
        )

    # ==================== æ¨¡å‹è°ƒç”¨æ–¹æ³• ====================

    async def _call_cloud_model(self, content: str, system_prompt: str, model_config) -> Optional[AIContentAnalysisResult]:
        """è°ƒç”¨äº‘ç«¯æ¨¡å‹"""
        if not model_config.api_key:
            logger.warning("äº‘ç«¯æ¨¡å‹ API_KEY ä¸ºç©ºï¼Œè·³è¿‡è°ƒç”¨")
            return None
        
        # äº‘ç«¯æ¨¡å‹ï¼šç›´æ¥ä½¿ç”¨ä¼ å…¥çš„æç¤ºè¯ï¼ˆå·²åœ¨è°ƒç”¨å¤„æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©ï¼‰
        # Promptå‹ç¼©å·²æ³¨é‡Šï¼Œä¸å†ä½¿ç”¨
        # try:
        #     from app.services.ai.prompt_compressor import prompt_compressor
        #     budget_status = token_optimizer.get_budget_status()
        #     daily_usage = budget_status.get("daily", {}).get("usage_rate", 0.0)
        #     
        #     if daily_usage > 0.8:
        #         strategy = "aggressive"
        #     elif daily_usage > 0.5:
        #         strategy = "moderate"
        #     else:
        #         strategy = "conservative"
        #     
        #     original_len = len(system_prompt)
        #     compressed_prompt = prompt_compressor.compress_prompt(system_prompt, strategy, model_type="cloud")
        #     compressed_len = len(compressed_prompt)
        #     savings = (original_len - compressed_len) / original_len * 100 if original_len > 0 else 0
        #     
        #     logger.info(f"[CloudPrompt] ğŸ“ Promptå‹ç¼©: ç­–ç•¥={strategy}, åŸå§‹={original_len}å­—ç¬¦, å‹ç¼©å={compressed_len}å­—ç¬¦, èŠ‚çœ={savings:.1f}%")
        # except Exception as e:
        #     logger.warning(f"[CloudPrompt] âš ï¸  Promptå‹ç¼©å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹Prompt: {e}")
        #     compressed_prompt = system_prompt
        compressed_prompt = system_prompt
        
        headers = {
            "Authorization": f"Bearer {model_config.api_key}",
            "Content-Type": "application/json",
        }

        messages = [
            {"role": "system", "content": compressed_prompt},
            {"role": "user", "content": f"è¾“å…¥å†…å®¹: {content}"},
        ]

        payload = {
            "model": model_config.name,
            "messages": messages,
            "temperature": 0.3,
            "max_tokens": 512,
        }

        try:
            async with httpx.AsyncClient(timeout=model_config.timeout) as client:
                resp = await client.post(
                    f"{model_config.base_url}/chat/completions",
                    json=payload,
                    headers=headers,
                )
                if resp.status_code != 200:
                    logger.error(f"Cloud Model API Error {resp.status_code}: {resp.text}")
                    await self._mark_metric("cloud_http_error")
                    if 400 <= resp.status_code < 500:
                        await self._mark_metric("cloud_http_4xx")
                    if 500 <= resp.status_code < 600:
                        await self._mark_metric("cloud_http_5xx")
                    if resp.status_code == 429:
                        await self._mark_metric("cloud_http_429")
                    return None
                
                response_text = resp.json()["choices"][0]["message"]["content"]
                data = self._parse_json_from_text(response_text)
                if not data:
                    logger.warning(f"Cloud JSON parse failed: {response_text[:300]}...")
                    await self._mark_metric("cloud_parse_error")
                    return None

                result = self.default_response.copy()
                result.update(data)
                if "confidence" not in result and isinstance(result.get("score"), (int, float)):
                    result["confidence"] = max(0.0, min(1.0, result["score"] / 100))
                result["source"] = "cloud_llm"
                result["model_name"] = model_config.name
                return result
                
        except Exception as e:
            logger.error(f"Cloud model call failed: {e}")
            await self._mark_metric("cloud_exception")
            return None

    async def _call_local_model(
        self,
        content: str,
        content_type: str,
        system_prompt: str,
        model_config
    ) -> Optional[AIContentAnalysisResult]:
        """è°ƒç”¨æœ¬åœ°æ¨¡å‹"""
        try:
            logger.info(f"[LLM] è°ƒç”¨æœ¬åœ°æ–‡æœ¬æ¨¡å‹: {model_config.name} @ {model_config.base_url}")
            local_result = await local_model_service.predict(
                content,
                content_type,
                system_prompt=system_prompt
            )
            if local_result:
                logger.info(f"[LocalLLM] score={local_result.get('score')}")

                final_result = {
                    "score": local_result.get("score", 60),
                    "category": local_result.get("category", "æ™®é€š"),
                    "label": local_result.get("label", "æ™®é€š"),
                    "reason": local_result.get("reason", "æœ¬åœ°æ¨¡å‹åˆ†æ"),
                    "is_highlight": local_result.get("is_highlight", False),
                    "is_inappropriate": local_result.get("is_inappropriate", False),
                    "confidence": local_result.get("confidence", 0.5),
                    "source": "local_model",
                    "model_name": model_config.name,
                }
                return final_result
            else:
                logger.warning("æœ¬åœ°æ¨¡å‹è¿”å›ç©ºç»“æœ")
                return None

        except Exception as e:
            logger.error(f"Local model call failed: {e}")
            return None

    async def evaluate_with_prompt(
        self,
        content: str,
        content_type: str,
        system_prompt: str,
        model_source: str = "auto"
    ) -> Optional[AIContentAnalysisResult]:
        """Run a one-off evaluation with a custom prompt, without caching."""
        source = (model_source or "auto").lower()
        if source == "local":
            model_config = model_registry.get_model("local_text")
            if not model_config:
                return None
            return await self._call_local_model(content, content_type, system_prompt, model_config)
        if source == "cloud":
            model_config = model_registry.get_model("cloud_text")
            if not model_config:
                return None
            return await self._call_cloud_model(content, system_prompt, model_config)

        for model_type in model_registry.get_text_model_priority():
            if model_type == "local_text":
                model_config = model_registry.get_model("local_text")
                if not model_config:
                    continue
                result = await self._call_local_model(content, content_type, system_prompt, model_config)
            else:
                model_config = model_registry.get_model("cloud_text")
                if not model_config:
                    continue
                result = await self._call_cloud_model(content, system_prompt, model_config)
            if result:
                return result
        return None

    # ==================== ç¼“å­˜è¾…åŠ© ====================

    async def _save_cache(
        self,
        content: str,
        content_type: str,
        result: AIContentAnalysisResult,
        embedding,
        prompt_version_id: Optional[int] = None
    ):
        try:
            content_hash = self._get_content_hash(content, prompt_version_id)
            exact_cache_key = f"ai:analysis:{content_type}:{content_hash}"
            sem_prefix = f"ai:semcache:{content_type}"

            # åªä¿å­˜ç²¾ç¡®ç¼“å­˜ï¼Œå»¶é•¿TTLåˆ°30å¤©
            cache_ttl = 30 * 24 * 3600  # 30å¤©
            await redis_service.async_redis.setex(
                exact_cache_key,
                cache_ttl,
                json.dumps(result, ensure_ascii=False),
            )
            
            # è¯­ä¹‰ç¼“å­˜ï¼šä»…å½“ embedding ä¸ä¸ºç©ºæ—¶ä¿å­˜
            if embedding:
                try:
                    # å¤ç”¨ search_similar_vector çš„ key è§„åˆ™
                    max_dim = settings.AI_VECTOR_DIMENSION
                    precision = settings.AI_VECTOR_QUANTIZATION_PRECISION
                    head = embedding[:min(len(embedding), max_dim)]
                    quantized = [f"{x:.{precision}f}" for x in head]
                    vector_key = "_".join(quantized)
                    sem_key = f"{sem_prefix}:{vector_key}"
                    await redis_service.async_redis.setex(
                        sem_key,
                        settings.AI_SEMANTIC_CACHE_TTL,
                        json.dumps(result, ensure_ascii=False),
                    )
                except Exception as e:
                    logger.warning(f"Semantic cache save failed: {e}")
            logger.debug(f"AIç²¾ç¡®ç¼“å­˜å·²ä¿å­˜: {content[:10]}... TTL={cache_ttl}s")
        except Exception as e:
            logger.warning(f"Cache save failed: {e}")

    # ==================== å¤šæ™ºèƒ½ä½“é™ªå®¡å›¢ï¼ˆå·²æ³¨é‡Šï¼Œæš‚æ—¶ä¸è€ƒè™‘ï¼‰ ====================
    # async def _maybe_use_jury(self, content: str, content_type: str, result: AIContentAnalysisResult, force_jury: bool = False) -> AIContentAnalysisResult:
    #     """
    #     æ ¹æ®é…ç½®å’Œç½®ä¿¡åº¦å†³å®šæ˜¯å¦è§¦å‘å¤šæ™ºèƒ½ä½“é™ªå®¡å›¢ã€‚
    #     """
    #     if not force_jury:
    #         return result
    #     if not settings.MULTI_AGENT_ENABLED:
    #         return result
    #     service = _get_multi_agent_service()
    #     if not service:
    #         return result
    #     try:
    #         jury_result = await service.analyze_with_jury(content, content_type)
    #         if jury_result:
    #             # ä¿ç•™åŸç»“æœä»¥ä¾¿è¿½è¸ª
    #             jury_result.setdefault("source", "multi_agent")
    #             jury_result.setdefault("confidence", result.get("confidence", 0.5))
    #             jury_result.setdefault("model_name", result.get("model_name"))
    #             trace = result.get("decision_trace", [])
    #             jury_trace = jury_result.pop("decision_trace", [])
    #             jury_result["decision_trace"] = trace + [{"step": "multi_agent"}] + jury_trace
    #             await self._mark_metric("jury_call")
    #             return jury_result
    #     except Exception as e:
    #         logger.warning(f"Multi-agent analyze failed: {e}")
    #     return result

    # ==================== è§„åˆ™è¿‡æ»¤ ====================

    def _rule_based_filter(
        self, content: str, content_type: str
    ) -> Optional[AIContentAnalysisResult]:
        clean = content.strip()
        if not clean:
            return self.default_response

        if re.match(r"^\d+$", clean):
            return {
                **self.default_response,
                "score": 45,
                "category": "çŒæ°´",
                "label": "å¤è¯»",
                "reason": "è§„åˆ™å‘½ä¸­ï¼šçº¯æ•°å­—",
            }

        if not re.search(r"[\u4e00-\u9fa5a-zA-Z0-9]", clean):
            return {
                **self.default_response,
                "score": 50,
                "category": "æƒ…ç»ªè¡¨è¾¾",
                "label": "è¡¨æƒ…",
                "reason": "è§„åˆ™å‘½ä¸­ï¼šçº¯ç¬¦å·/è¡¨æƒ…",
            }

        if len(clean) < 2:
            return {
                **self.default_response,
                "score": 40,
                "category": "æ— æ„ä¹‰",
                "label": "è¿‡çŸ­",
                "reason": "è§„åˆ™å‘½ä¸­ï¼šå†…å®¹è¿‡çŸ­",
            }

        keywords = [
            k.strip()
            for k in settings.AI_LOW_VALUE_KEYWORDS.split(",")
            if k.strip()
        ]
        if any(k in clean for k in keywords):
            return {
                **self.default_response,
                "score": 50,
                "category": "æƒ…ç»ªè¡¨è¾¾",
                "reason": "è§„åˆ™å‘½ä¸­ï¼šä½ä»·å€¼å…³é”®è¯",
            }

        return None

    # ==================== Prompt ====================

    def _build_prompt(self, content: str, content_type: str) -> list:
        system_prompt = (
            DANMAKU_SYSTEM_PROMPT
            if content_type == "danmaku"
            else COMMENT_SYSTEM_PROMPT
        )
        return [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"è¾“å…¥å†…å®¹: {content}"},
        ]

    # ==================== å›¾åƒåˆ†æåŠŸèƒ½ ====================

    async def analyze_image(self, image_data: str, prompt: str = "è¯·åˆ†æè¿™å¼ å›¾ç‰‡çš„å†…å®¹") -> Optional[str]:
        """
        å›¾åƒåˆ†æåŠŸèƒ½
        
        Args:
            image_data: base64ç¼–ç çš„å›¾åƒæ•°æ®
            prompt: åˆ†ææç¤ºè¯
            
        Returns:
            åˆ†æç»“æœæ–‡æœ¬ï¼Œå¤±è´¥è¿”å›None
        """
        if not self.use_cloud:
            logger.warning("å›¾åƒåˆ†æéœ€è¦äº‘ç«¯æ¨¡å‹æ”¯æŒï¼Œå½“å‰æ¨¡å¼æœªå¯ç”¨äº‘ç«¯")
            return None
        if not self.vision_api_key:
            logger.warning("LLM_VISION_API_KEY ä¸ºç©ºï¼Œè·³è¿‡å›¾åƒåˆ†æ")
            return None
            
        if not self.vision_model:
            logger.warning("æœªé…ç½®å›¾åƒè¯†åˆ«æ¨¡å‹")
            return None

        headers = {
            "Authorization": f"Bearer {self.vision_api_key}",
            "Content-Type": "application/json",
        }

        payload = {
            "model": self.vision_model,
            "messages": [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": prompt
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/png;base64,{image_data}"
                            }
                        }
                    ]
                }
            ],
            "temperature": 0.3,
            "max_tokens": 1000
        }

        try:
            async with httpx.AsyncClient(timeout=60.0) as client:  # å›¾åƒå¤„ç†éœ€è¦æ›´é•¿æ—¶é—´
                resp = await client.post(
                    f"{self.vision_base_url}/chat/completions",
                    json=payload,
                    headers=headers,
                )
                if resp.status_code != 200:
                    logger.error(f"Vision API Error {resp.status_code}: {resp.text}")
                    return None
                return resp.json()["choices"][0]["message"]["content"]
        except Exception as e:
            logger.error(f"Vision API call failed: {e}")
            return None

    # ==================== LLM API è°ƒç”¨ ====================

    async def _call_llm_api(self, messages: list) -> Optional[str]:
        """
        äº‘ç«¯å¤§æ¨¡å‹APIè°ƒç”¨
        """
        if not self.api_key:
            logger.warning("LLM_API_KEY ä¸ºç©ºï¼Œè·³è¿‡äº‘ç«¯è°ƒç”¨")
            return None
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }

        payload = {
            "model": self.model,
            "messages": messages,
            "temperature": 0.3,
            "max_tokens": 512,
        }

        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                resp = await client.post(
                    f"{self.base_url}/chat/completions",
                    json=payload,
                    headers=headers,
                )
                if resp.status_code != 200:
                    logger.error(f"LLM API Error {resp.status_code}: {resp.text}")
                    return None
                return resp.json()["choices"][0]["message"]["content"]
        except Exception as e:
            logger.error(f"LLM API call failed: {e}")
            return None

    # ==================== è§£æ ====================

    def _parse_json_from_text(self, text: str) -> Optional[dict]:
        """
        ä»æ¨¡å‹è¾“å‡ºä¸­å°½é‡æå– JSON å¯¹è±¡ã€‚
        å…¼å®¹ï¼š```json ... ``` + é¢å¤–è¯´æ˜/Markdownã€‚
        """
        if not text:
            return None

        def _cleanup(candidate: str) -> str:
            c = (candidate or "").strip()
            # å…¼å®¹æ¨¡å‹è¾“å‡ºçš„â€œå°¾é€—å·â€
            c = re.sub(r",\s*([}\]])", r"\1", c)
            return c

        # 1) ä¼˜å…ˆè§£æ fenced code blockï¼ˆé€šå¸¸æœ€å¹²å‡€ï¼‰
        try:
            for m in re.finditer(r"```(?:json)?\s*([\s\S]*?)\s*```", text, flags=re.IGNORECASE):
                candidate = _cleanup(m.group(1) or "")
                if not candidate:
                    continue
                obj = json.loads(candidate)
                if isinstance(obj, dict):
                    return obj
        except Exception:
            pass

        # 2) å»é™¤ code fence åç›´æ¥ json.loads
        clean_text = _cleanup(re.sub(r"```json\s*|\s*```", "", text, flags=re.IGNORECASE))
        try:
            obj = json.loads(clean_text)
            if isinstance(obj, dict):
                return obj
        except Exception:
            pass

        # 3) å†é€€åŒ–ï¼šæˆªå–ç¬¬ä¸€ä¸ª {...} åŒºé—´ï¼ˆä¸ local_model_service åŒç­–ç•¥ï¼‰
        try:
            start_obj = clean_text.find("{")
            end_obj = clean_text.rfind("}")
            if start_obj != -1 and end_obj != -1 and end_obj > start_obj:
                obj = json.loads(clean_text[start_obj : end_obj + 1])
                if isinstance(obj, dict):
                    return obj
        except Exception:
            pass

        return None

    def _parse_response(
        self, response_text: str, content_type: str
    ) -> AIContentAnalysisResult:
        try:
            data = self._parse_json_from_text(response_text) or {}
            result = self.default_response.copy()
            result.update(data)
            # è¡¥å……ç¼ºçœå­—æ®µ
            if "confidence" not in result and isinstance(result.get("score"), (int, float)):
                result["confidence"] = max(0.0, min(1.0, result["score"] / 100))
            return result
        except Exception:
            logger.warning(f"JSON parse error: {response_text}")
            return self.default_response

    # ==================== å¼‚æ­¥ä»»åŠ¡ ====================

    async def process_danmaku_task(self, danmaku_id: int):
        # é˜Ÿåˆ—æ¨¡å¼ï¼šå¿«é€Ÿå…¥é˜Ÿï¼Œé¿å…é«˜å¹¶å‘æ—¶ç½‘ç»œ/IO é˜»å¡
        if self.queue_enabled:
            await self.enqueue_analysis("danmaku", danmaku_id, priority="low")
            return

        await self._process_danmaku_task_immediate(danmaku_id)

    def _effective_priority(self, content_type: str, content: str, priority: str) -> str:
        """æ ¹æ®å†…å®¹ç‰¹å¾ä¿®æ­£ä¼˜å…ˆçº§ï¼Œé¿å…å…³é”®å†…å®¹è¢«é‡‡æ ·è·³è¿‡ã€‚"""
        clean = (content or "").strip()
        if content_type == "danmaku":
            # é•¿å¼¹å¹•æ›´å¯èƒ½éœ€è¦è¯­ä¹‰ç¼“å­˜/æœ¬åœ°æ¨¡å‹åˆ†æï¼›çŸ­å¼¹å¹•ä»å¯é‡‡æ ·è·³è¿‡
            min_len = int(getattr(settings, "DANMAKU_ANALYSIS_MIN_LEN", 20) or 20)
            if len(clean) >= min_len:
                return "high"
        return priority

    async def _process_danmaku_task_immediate(self, danmaku_id: int):
        db = SessionLocal()
        try:
            danmaku = db.query(Danmaku).filter(Danmaku.id == danmaku_id).first()
            if not danmaku:
                return

            priority = self._effective_priority("danmaku", danmaku.content, "low")
            result = await self.analyze_content_with_policy(
                danmaku.content,
                "danmaku",
                force_jury=False,
                priority=priority,
            )
            danmaku.ai_score = result.get("score", 60)
            danmaku.ai_category = result.get("category", "æ™®é€š")
            danmaku.ai_reason = result.get("reason")
            danmaku.ai_confidence = result.get("confidence", 0.5)
            danmaku.ai_source = result.get("source")
            danmaku.ai_prompt_version_id = result.get("prompt_version_id")
            danmaku.ai_model = result.get("model_name")
            trace = result.get("decision_trace")
            if trace and self._should_store_trace(result):
                try:
                    danmaku.ai_trace = json.dumps(trace, ensure_ascii=False)
                except Exception:
                    danmaku.ai_trace = None
            danmaku.is_highlight = (
                result.get("is_highlight", False) or danmaku.ai_score >= 90
            )
            db.commit()
        except Exception as e:
            logger.error(f"Danmaku task error: {e}")
            db.rollback()
        finally:
            db.close()

    async def process_comment_task(self, comment_id: int):
        # é˜Ÿåˆ—æ¨¡å¼ï¼šå¿«é€Ÿå…¥é˜Ÿï¼Œé¿å…é«˜å¹¶å‘æ—¶ç½‘ç»œ/IO é˜»å¡
        if self.queue_enabled:
            await self.enqueue_analysis("comment", comment_id, priority="low")
            return

        await self._process_comment_task_immediate(comment_id)

    async def _process_comment_task_immediate(self, comment_id: int):
        db = SessionLocal()
        try:
            comment = db.query(Comment).filter(Comment.id == comment_id).first()
            if not comment:
                return

            result = await self.analyze_content_with_policy(comment.content, "comment", force_jury=False, priority="low")
            comment.ai_score = result.get("score", 60)
            comment.ai_label = result.get("label", "æ™®é€š")
            comment.ai_reason = result.get("reason")
            # æ•´å‹å­˜å‚¨ï¼šä¹˜ä»¥ 100
            conf = result.get("confidence", 0.5)
            comment.ai_confidence = int(conf * 100) if conf is not None else None
            comment.ai_source = result.get("source")
            comment.ai_prompt_version_id = result.get("prompt_version_id")
            comment.ai_model = result.get("model_name")
            trace = result.get("decision_trace")
            if trace and self._should_store_trace(result):
                try:
                    comment.ai_trace = json.dumps(trace, ensure_ascii=False)
                except Exception:
                    comment.ai_trace = None
            db.commit()
        except Exception as e:
            logger.error(f"Comment task error: {e}")
            db.rollback()
        finally:
            db.close()


# ==================== å…¨å±€å®ä¾‹ ====================

llm_service = LLMService()
